# Z3-2.3 — Data Quality & Validation as Code

**(testes unitários para dados, como se fossem código de produção)**

Na Z3 a gente para de tratar qualidade de dados como “boa vontade do time de BI” e passa a tratá-la como **parte do pipeline de software**, com testes automatizados, versionados e auditáveis.

Princípio central:

> **Se o dado não passou nos testes, ele não é promovido para a Curated/Feature Store.
> DQ é *gate de mudança*, não relatório depois do estrago.**

---

## 1) Objetivo

Transformar regras de qualidade, consistência e integridade de dados em:

* **Especificações explícitas** (Expectations/Constraints),
* **Código versionado** (no Git, revisado via PR),
* **Checks automáticos em cada execução de pipeline**,
* **Evidências de execução** (logs, relatórios, métricas em Z9).

Isso está 100% alinhado com a ideia de *data unit tests* de frameworks como **Great Expectations** e **Deequ**, que tratam qualidade de dados como testes automatizados aplicados em grandes volumes.

---

## 2) Tipos de Regras de Qualidade (o que checar na Z3)

As regras são construídas por dataset (Curated) e, quando relevante, por *feature set*.

**Categorias principais:**

1. **Estruturais**

   * Colunas obrigatórias presentes.
   * Tipos corretos (string, int, date…).
   * Campos NOT NULL respeitados.
   * Chaves primárias únicas (sem duplicatas indevidas).
   * Integridade referencial (chaves estrangeiras válidas).

2. **Regras de negócio**

   * Valores numéricos em faixas plausíveis:

     * `amount >= 0`, `score` entre 0 e 1000.
   * Regras de consistência:

     * `data_fim >= data_inicio`,
     * `status` em (`ATIVO`, `INADIMPLENTE`, `ENCERRADO`),
     * transação de estorno sempre ligada a uma transação original existente.
   * Regras regulatórias:

     * campos obrigatórios para crédito, PLD/FT, KYC preenchidos.

3. **Qualidade estatística & distribuição**

   * Taxa máxima de nulos por coluna,
   * Distribuição de valores (média, desvio padrão, quantis),
   * Detecção de **outliers** e **mudanças bruscas** (data drift de entrada):

     * ex.: transações diárias caem 80% sem motivo,
     * explosão de um valor específico (pode ser ataque ou bug de origem).

4. **Regras de privacidade & segurança**

   * Campos marcados como `pii=true` não aparecem em datasets `public`.
   * Não existem CPFs em colunas que não são de PII.
   * Colunas `*_token` são realmente tokenizadas (regex/forma esperada).
   * Não há dados sensíveis em colunas de *log* / *free text*.

5. **Regras de integridade técnica**

   * Hashs dos arquivos/lotes batem com o registrado em Z2 (2.6).
   * Não há linhas parcialmente truncadas, encoding inválido, JSON quebrado, etc.

Essas regras são definidas como código, não como “texto no Confluence”.

---

## 3) Data Quality como Código (Expectations / Constraints)

Pense em cada regra de qualidade como um “teste unitário”:

```yaml
expectations:
  - name: "transaction_id_unique"
    type: uniqueness
    column: transaction_id
    severity: error

  - name: "amount_is_non_negative"
    type: value_range
    column: amount
    min: 0
    severity: error

  - name: "status_in_allowed_values"
    type: allowed_values
    column: status
    values: ["APPROVED", "DECLINED", "PENDING"]
    severity: error

  - name: "cpf_is_tokenized"
    type: regex_match
    column: cpf_token
    regex: "^[A-F0-9]{64}$"
    severity: error
```

Esses arquivos:

* vivem em `dq/<dataset>/v<schema_version>.yaml`,
* são versionados junto com o código do pipeline,
* passam por **code review**,
* são executados em cada *run* de curadoria.

Ferramentas de referência (para citar na entrevista):

* **Great Expectations (GX)** — expectations como código + docs automáticas.
* **AWS Deequ** — “unit tests for data” em cima de Spark.

No lab, você pode não usar tudo isso “full”, mas pode simular:

* scripts Python com checks,
* saída em JSON/HTML com resultado por regra,
* falhando o job de ETL se algo crítico quebrar.

---

## 4) Gate de Promoção: como o DQ protege a Curated/Feature Store

A lógica é simples (e poderosa):

1. **Job lê da Z2 (Raw) ou Staging.**

2. **Transforma** dados para o formato Curated.

3. **Executa a suíte DQ (Z3-2.3)**:

   * Cada regra retorna:

     * `PASS`, `WARN` ou `FAIL`.

4. **Decisão automática**:

   * Se houver qualquer `FAIL` crítico:

     * **não grava** na Curated,
     * manda lote para **Quarentena** (Z2-2.7),
     * gera alerta (Z9),
     * abre issue / notificação pro time.
   * Se houver apenas `WARN`:

     * grava com flag `data_quality_status=warning`,
     * registra no relatório DQ, notifica owner.
   * Se tudo `PASS`:

     * promove dataset normalmente,
     * atualiza Catálogo/Lineage com link pro relatório.

5. **Regra de ouro (pra você falar na entrevista)**:

   > “Nenhum dataset entra na Curated/Feature Store sem evidência de DQ executada. Sem evidência, não é confiável.”

---

## 5) Integração com Z1, Z2, Z4 e Feature Store

**Z1 — Validação inicial (leve)**

* Já filtra lixo óbvio: MIME, schema básico, tamanho, etc.
* Evita que dados totalmente inválidos sequer cheguem na Z2.
  (alinhado com NIST SP 800-53 SI-10 — *Information Input Validation*).

**Z2 — Coerência com Raw**

* DQ na Z3 usa metadados/hashes da Z2 (2.6) pra garantir:

  * integridade do que está sendo curado,
  * coerência entre o que foi ingerido e o que está sendo publicado.

**Z4 — Model Factory**

* Os pipelines de treino:

  * **não escolhem aleatoriamente** “tabela X”.
  * Eles referenciam datasets curados + versões + relatórios DQ.
* Falha de DQ → bloqueia ou marca o treino como inválido (evita treinar modelo em dado ruim → mitiga ML01/ML02 de OWASP ML Top 10).

**Feature Store (Z3-2.6)**

* Features derivadas **herdam** os resultados de DQ das tabelas de origem.
* Pode haver **Expectations específicas de features**:

  * ex.: `score_fraude` ∈ [0, 1],
  * `days_since_last_tx` ≥ 0,
  * distribuição estável vs histórico (detecção de drift).

---

## 6) Segurança, Compliance & DQ

Data Quality não é só sobre “dados bonitos”; é controle de segurança e compliance também.

**Riscos de segurança que DQ como código ajuda a mitigar:**

* **Data Poisoning “limpo”**:

  * Atacante envia dados sintaticamente válidos, mas semanticamente maliciosos.
  * Checks de distribuição, volume, consistência histórica detectam:

    * surtos anômalos,
    * correlações irreais,
    * campos invertidos, etc.
* **Software & Data Integrity Failures (OWASP A08:2021)**:

  * DQ + hashes + lineage ajudam a detectar modificações indevidas em dados críticos.
* **Quebra de contrato de dados / schema**:

  * Alinhado a NIST SP 800-53 (SI-10, CM-3/CM-6) — validação de entrada e controle de mudanças.
* **Violação de privacidade (LGPD/GDPR)**:

  * Checks automáticos para garantir:

    * ausência de PII onde não deveria existir,
    * aplicação correta de mascaramento/tokenização.

**Mensagem boa pro entrevistador:**

> “Eu enxergo Data Quality como camada de segurança e governança: expectativas automáticas para pegar não só erro de código, mas também sinais de poisoning, schema smuggling e vazamento de PII.”

---

## 7) Implementação no MLOps Security Lab (como você mostra isso na prática)

Você não precisa montar um mega data quality platform agora, mas pode demonstrar o conceito:

1. **Definir Expectations por dataset**

   * Criar arquivos YAML/JSON para:

     * `curated_risco_transacoes_v1`,
     * `curated_clientes_v1`, etc.
   * Incluir regras de integridade, ranges, PII, etc.

2. **Script de Validação**

   * Em Python (com `pandas` ou `Great Expectations`) ou Spark:

     * lê o dataset gerado,
     * carrega o YAML de expectations,
     * executa os checks,
     * gera um relatório (JSON/HTML) com PASS/WARN/FAIL.

3. **Integração com Airflow**

   * Task `run_dq_checks` após `transform`.
   * Se houver `FAIL crítico`:

     * marcar DAG como `failed`,
     * não executar `load_to_curated`,
     * enviar notificação (Slack/Email/Log).

4. **Persistir Evidências**

   * Guardar relatórios DQ em:

     * `dq_reports/<dataset>/<run_id>.json`.
   * Gravar no metadado da partição curada:

     * `dq_status`, `dq_report_ref`.

5. **Dashboards simples**

   * No Grafana/Kibana:

     * % de runs com PASS/WARN/FAIL,
     * principais regras que mais quebram,
     * evolução da qualidade por dataset.

---

## 8) Anti-patterns para evitar (e apontar)

* Rodar DQ “uma vez por mês” manualmente.
* Ter regras só na cabeça de uma pessoa (“fulano confere”).
* Tratar violação de regra crítica como log de aviso e seguir a carga.
* Não versionar regras: mudar a regra e reescrever o passado sem rastro.
* Não alinhar DQ com segurança/privacidade (só olhar null/blank, ignorar PII, leakage, etc.).

---

## 9) Checklist rápido Z3-2.3

Antes de dizer que um dataset é “Curated” de verdade:

* [ ] Existe arquivo de **schema** (Z3-2.2) versionado.
* [ ] Existe **suíte DQ** formal (Expectations/Constraints) versionada.
* [ ] O pipeline **executa DQ automaticamente** em todo run.
* [ ] Falhas críticas de DQ **bloqueiam promoção** para Curated/Feature Store.
* [ ] Relatórios DQ são armazenados e referenciados (lineage/evidência).
* [ ] Há checks específicos para **PII, integridade, ranges e distribuições**.
* [ ] Logs e métricas de DQ são enviados para **Z9** (para monitorar saúde dos dados).
