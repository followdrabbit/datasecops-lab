# Z3-2.2 — Data Modeling & Schema Registry

**(onde o dado ganha forma, contrato e governança)**

Se a Curated Zone (Z3-2.1) é “o dado já limpo e confiável”, o **Data Modeling & Schema Registry** é o mecanismo que garante que isso **não é acidental**, e sim repetível, auditável e previsível.

Sem modelo e sem registry, a Curated vira outro Raw “um pouco mais bonito”.
Com eles, você tem:

* **Contratos claros** entre quem produz e quem consome dados.
* **Evolução segura de schema**, sem quebrar pipelines ou modelos em produção.
* **Visibilidade e governança** sobre o que cada coluna significa, quem é dono e o quão sensível ela é.
* **Alinhamento com segurança**, compliance, OWASP/API Sec, NIST/CSA, etc.

---

## 1) Objetivo

> **Transformar o schema em ativo de primeira classe, versionado, revisado e aplicado automaticamente em todos os pipelines.**

Em termos práticos:

* Garantir que cada dataset curado (Z3-2.1) seja descrito por um **contrato formal de dados**.
* Fornecer um **repositório único (Registry)** onde esses contratos vivem, com histórico, versões, owners, aprovações.
* Integrar esse registry com:

  * pipelines de ingestão (Z1/Z2),
  * curadoria (Z3),
  * Feature Store (Z3-2.6),
  * Model Factory (Z4),
  * Governança (Z8/Z9).

---

## 2) Princípios de Modelagem na Z3

1. **Orientado a Domínios (Domain-Driven + Data Product)**
   Cada dataset curado pertence a um domínio (ex.: `risco`, `fraude`, `credito`, `pagamentos`, `clientes`) com dono claro.

2. **Contratos Explícitos, Não Implícitos**
   Nada de “o schema é o que está na tabela hoje”.
   O schema vive em arquivo versionado + registry, referenciado pelos pipelines.

3. **Evolução Controlada**
   Mudanças em schema seguem regras:

   * **compatíveis**: adicionam; não quebram consumidores;
   * **quebrando**: exigem nova versão (`v2`, `v3`) e plano de migração.

4. **Privacidade & Segurança Embutidas no Modelo**
   Tipos, constraints e *tags de sensibilidade* fazem parte do schema (não são só planilha solta).

5. **“Schema as Code” + “Policy as Code”**
   Schemas e políticas de acesso/validação vivem no Git + CI/CD:

   * PRs revisados,
   * testes automáticos,
   * histórico auditável.

---

## 3) Elementos do Data Model (o que um schema precisa trazer)

Para cada dataset curado na Z3, o schema deve incluir:

* **Estrutura**

  * Nome da tabela/dataset (ex.: `curated_risco_transacoes_v2`).
  * Colunas com:

    * nome,
    * tipo (ex.: `string`, `decimal(18,2)`, `timestamp`),
    * tamanho/precisão,
    * nulabilidade (NOT NULL?),
    * chaves primárias e estrangeiras.
* **Semântica**

  * descrição clara do que cada campo significa,
  * exemplos de valores válidos,
  * domínio (`enum`, ranges permitidos).
* **Metadados de Governança**

  * `data_owner` (time responsável),
  * `steward` (quem cuida do dia a dia),
  * `sla_freshness` (ex.: atualiza a cada 15min),
  * `rpo/rto` relevantes.
* **Classificação & Privacidade**

  * `sensitivity`: `public`, `internal`, `confidential`, `restricted`, `pii`, `secret`.
  * flags: `contains_pii`, `contains_card_data`, `contains_credentials`, etc.
* **Regras de Qualidade & Negócio (links)**

  * referência para a suite de DQ (Z3-2.3),
  * constraints lógicos (ex.: `amount >= 0`, `birth_date <= now-18y`).
* **Lineage**

  * referências a:

    * datasets de origem em Z2 (`raw/...`),
    * transformações relevantes (join, agregação, filtros),
    * versão do pipeline/código que gera o dataset.

Isso vira um **artefato estruturado** (YAML/JSON/Avro/DDL) mantido no repositório do projeto.

---

## 4) O que é o Schema Registry (na prática)

O **Schema Registry** é o sistema (ou serviço) que:

* Guarda todos os schemas versionados dos datasets, eventos e features.
* Expõe APIs/UI para:

  * registrar nova versão,
  * consultar schema vigente,
  * validar payloads/lotes contra o schema.
* Atua como **“fonte da verdade”** para qualquer componente que escreva ou leia dados.

No contexto do laboratório, você pode:

* Implementar como:

  * diretório `schemas/` versionado no Git + uma **API simples** de leitura,
  * ou integrar com algo tipo **Confluent Schema Registry** (se usar Avro/Protobuf),
  * ou um serviço leve (FastAPI) que lê os YAML/JSON de schema do repositório.

---

## 5) Ciclo de Vida de um Schema (Governança)

Modelar schema na Z3 é um mini SDLC:

1. **Proposição**

   * Time de domínio abre PR com novo schema ou mudança.
   * Define:

     * semântica,
     * tipos,
     * PII,
     * owner,
     * DQ mínima.

2. **Validação Automática (CI)**

   * Linters: formato do YAML/JSON/DDL.
   * Regras de compatibilidade:

     * se mudar tipo/chave/remover campo → marcar como **major change**.
   * Regras de segurança:

     * campos marcados como `pii` não podem ir pra dataset “public”.
     * certos tipos só permitidos em zonas `restricted`.

3. **Revisão & Aprovação**

   * Revisão por:

     * time de dados,
     * segurança,
     * eventualmente jurídico/privacidade (para PII).
   * Comentários, ajustes.

4. **Publicação no Registry**

   * Ao *merge*:

     * nova versão é publicada no Schema Registry,
     * pipelines de ingestão/curadoria passam a usar o novo schema em ambiente de teste,
     * após validação → produção.

5. **Deprecação de Versões Antigas**

   * Para mudanças breaking:

     * manter v1 e v2 em paralelo,
     * comunicar consumidores,
     * definir data de desligamento do v1,
     * após prazo → bloquear novas gravações no v1.

---

## 6) Validação de Schema na Esteira de Dados

O Schema Registry só é útil se for **usado em tempo real**.

Pontos de integração:

1. **Na Z1 (Content Validation)**

   * Endpoints de ingestão conhecem o schema esperado:

     * validam campos obrigatórios,
     * tipos básicos,
     * formato (ex.: JSON vs CSV),
     * rejeitam payload inválido antes de chegar na Z2.

2. **Na Z2 (Entrada na Raw)**

   * Conectores oficiais:

     * verificam que o lote está conforme o schema “raw” esperado (mínimo).
   * Gravou algo fora do contrato?

     * vai pra Staging/Quarentena.

3. **Na Z3 (Curadoria)**

   * Jobs de transformação:

     * **sempre** validam o output contra o schema curado do registry.
   * Se não bater:

     * falham o job,
     * não promovem o dataset,
     * abrem alerta (Z9).

4. **Na Feature Store & Model Training**

   * Pipelines de treino consultam o registry:

     * pra saber como montar features,
     * garantir que colunas e tipos batem,
     * registrar (no Model Registry / Z5) a versão de schema usada.

---

## 7) Evolução de Schema com Segurança

Pontos que valem ouro em entrevista e no lab:

* **Adição de coluna opcional** → *backward compatible*:

  * consumidores antigos continuam funcionando.
* **Mudança de tipo compatível** (ex.: `int` → `bigint`, aumentar tamanho de string) → possível com cuidado.
* **Mudanças breaking**:

  * renomear/remover coluna,
  * mudar semântica (ex.: `status` muda de [A,B] pra [1,2,3]),
  * precisam de:

    * nova versão do dataset (`*_v2`),
    * migração planejada,
    * comunicação aos consumidores.

**Nunca**:

* alterar tabela em produção “no feeling” sem atualizar o schema e a documentação.
* sobrescrever significado de coluna mantendo o mesmo nome (gera bug fantasma em modelos).

---

## 8) Segurança & Privacidade integradas ao Schema

O Schema Registry é ponto perfeito pra embutir políticas de segurança:

* Campos com `pii=true`:

  * só podem aparecer em datasets com `classification >= restricted`.
  * gatilhos geram alerta se alguém tentar introduzir PII em dataset `public`.
* Marcação de:

  * `masking_policy`: como mascarar em queries (ex.: mostrar só últimos 4 dígitos).
  * `encryption_required`: se deve ser criptografado em nível de coluna.
* Integração com IAM/ABAC (Z8):

  * policies dinâmicas baseadas em atributos do schema.
  * Ex.: “usuários com role `analyst` não podem selecionar colunas `pii=true`”.

Isso mostra maturidade: **segurança e privacidade by design, no modelo** — não como gambiarra depois.

---

## 9) Implementação no MLOps Security Lab

Você pode montar uma versão simplificada, mas pedagógica:

1. **Pasta `schemas/` no repositório**

   * ex.: `schemas/curated/risco_transacoes/v1/schema.yaml`
   * com:

     * colunas, tipos, PII, owner, etc.

2. **Validador em CI**

   * Script Python que:

     * lê todos os YAMLs,
     * valida sintaxe,
     * aplica regras (ex.: nenhuma coluna PII em dataset `public`),
     * falha o PR se algo violar as regras.

3. **Validador nos DAGs de Curadoria**

   * Antes de gravar `curated/...`:

     * carregar schema do arquivo/registry,
     * checar colunas/tipos/not-null,
     * abortar se divergente.

4. **(Opcional) Mini Schema Registry API**

   * Um pequeno serviço que:

     * expõe `GET /schemas/curated/risco_transacoes/v1`,
     * usado por pipelines e pela Feature Store.

5. **Documentação**

   * Cada schema com:

     * README curto,
     * exemplos de uso,
     * link para o dataset/Feature correspondente.

---

## 10) Riscos mitigados (link com Z3 – seção 3)

Com Data Modeling + Schema Registry bem implantados, você reduz:

* **3.1 — Garbage In / Garbage Model**

  * impedindo schema quebrado e dados tortos chegarem na Curated.
* **3.2 — Vazamento de PII**

  * marcando campos sensíveis e bloqueando usos inadequados já no PR/schema.
* **3.3 — Target/Signal Leakage**

  * deixando claro no modelo quais campos são labels, quais são features, quais não podem ir pro online.
* **3.4 — Bypass da Z3**

  * consumidores preferem Z3 porque ela é previsível e bem documentada.
* **3.5 — Falta de Lineage**

  * schemas + metadados apontam para Z2 e para os pipelines.
* **3.6 — Supply chain de features**

  * somente fontes e transformações com contrato entram na Feature Store.
